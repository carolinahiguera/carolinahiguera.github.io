<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Carolina Higuera</title>
  
  <meta name="author" content="Carolina Higuera">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Carolina Higuera</name>
                                    </p>
                                    <p>I am a 4th-year Ph.D. student in the Paul G. Allen School at the University of Washington, where I am advised by <a href="https://homes.cs.washington.edu/~bboots/">Byron Boots</a>. Additionally, I am a visiting researcher at Meta, working with <a href="https://www.mustafamukadam.com/">Mustafa Mukadam</a> as part of the AIM mentorship program.
                                        
                                    </p>
                                    
                                    <p>
                                        I received my M.S. in Electronics Engineering from Universidad de los Andes, Bogota, Colombia. There, I worked on multiagent reinforcement learning for traffic light signal control.
                                    </p>

                                    <p>
                                        I am honored to be a Fulbright scholar from Colombia &#127464;&#127476; 
                                    </p>

                                    <p>
                                        Contact: 
                                        <ul>
                                            <li>chiguera [at] cs [dot] washington [dot] edu</li>
                                            <li>carohiguera [at] meta [dot] com</li>
                                        </ul>

                                    </p>
                                    <p align=center>
                                        <a href="mailto:chiguera@cs.washington.edu">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=ZaxycbsAAAAJ&hl=en" target="_blank">Google Scholar</a>
                                        <!-- &nbsp/&nbsp -->
                                        <!-- <a href="https://www.linkedin.com/in/anthonysimeonov/" target="_blank">LinkedIn</a> -->
                                    </p>
                                    
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <img style="width:100%;max-width:100%" alt="profile photo" src="images/cha_profile.png">
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        My research focuses on developing models that enable robot manipulators to interpret their environment through vision-based tactile perception. Specifically, I've been exploring how tracking extrinsic contacts between objects and their environment can be advantageous for policy learning. 
                                        
                                        However, to truly leverage touch in robotics, we need a common backbone. Currently, I'm working on learning tactile representations using self-supervised learning that can be versatile for both static and dynamic contact interactions via vision-based tactile sensing
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Updates</heading>
                                    <ul>
                                        <li>Visitor Researcher 
                                            at FAIR and GUM team,  advised by Mustafa Mukadam.</li>
                                        <li>Summer 2023 research internship at FAIR with the GUM team and advised by Mustafa Mukadam.</li>
                                        <li>NCF paper accepted at ICRA 2023</li>
                                      </ul>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                   

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Publications</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:10px;width:30%;vertical-align:middle">
                                    <img src='images/sparsh.gif' width="100%" />
                                    <!--<img src='images/mug_cut.gif' width="100%" />-->
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://ai.meta.com/research/publications/sparsh-self-supervised-touch-representations-for-vision-based-tactile-sensing/">
                                        <papertitle>Sparsh: Self-supervised touch representations for vision-based tactile sensing</papertitle>
                                    </a>
                                    <br>
                                    <strong>Carolina Higuera*</strong>,
                                    <a>Akash Sharma</a>,
                                    <a>Chaithanya Krishna Bodduluri</a>,
                                    <a>Taosha Fan</a>,
                                    <a>Patrick Lancaster</a>,
                                    <a>Mrinal Kalakrishnan</a>,
                                    <a>Michael Kaes</a>,
                                    <a>Byron Boots</a>,
                                    <a>Mike Lambeta</a>,
                                    <a>Tingfan Wu</a>,
                                    <a>Mustafa Mukadam</a>
                                    <br>
                                    <em>CoRL, 2024</em>
                                    <br>
                                    <a href="https://github.com/facebookresearch/sparsh">[code]</a>
                                    <!-- <p style="color:rgb(229, 118, 8);">
                                        Best Paper Award, ICRA 2023 Workshop on Effective Representations, Abstractions, and Priors for Robot Learning, <a href="https://sites.google.com/view/rap4robots">(RAP4Robots)</a>
                                    </p> -->
                                    <p></p>
                                    <p>
                                        Sparsh is a family of general touch representations trained via self-supervision algorithms such as MAE, DINO and JEPA. Sparsh is able to generate useful representations for DIGIT, Gelsight'17 and Gelsight Mini. It outperforms end-to-end models in the downstream tasks proposed in TacBench by a large margin, and can enable data efficient training for new downstream tasks.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/ncf_v2_demo.gif' width="100%" />
                                    <!--<img src='images/mug_cut.gif' width="100%" />-->
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2309.16652">
                                        <papertitle>Perceiving Extrinsic Contacts from Touch Improves Learning Insertion Policies</papertitle>
                                    </a>
                                    <br>
                                    <strong>Carolina Higuera*</strong>,
                                    <a>Joseph Ortiz</a>,
                                    <a>Haozhi Qi</a>
                                    <a>Luis Pineda</a>
                                    <a>Byron Boots</a>,
                                    <a>Mustafa Mukadam</a>
                                    <br>
                                    <em>arXiv:2309.16652, 2023</em>
                                    <br>
                                    <a href="https://github.com/carolinahiguera/NCF-Policy">[code]</a>
                                    <br>
                                    <p></p>
                                    <p>
                                        Improve NCF to enable sim-to-real transfoer and use it to train policies for insertion tasks. We demonstrate the utility of extrinsisc contacts during policy learning and perform experiments on a real tasks.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/digit.gif' width="100%" />
                                    <!--<img src='images/mug_cut.gif' width="100%" />-->
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2304.01182">
                                        <papertitle>Learning to Read Braille: Bridging the Tactile Reality Gap with Diffusion Models</papertitle>
                                    </a>
                                    <br>
                                    <strong>Carolina Higuera*</strong>,
                                    <a>Byron Boots</a>,
                                    <a>Mustafa Mukadam</a>
                                    <br>
                                    <em>arXiv:2304.01182, 2023</em>
                                    <br>
                                    <a href="https://github.com/carolinahiguera/Tactile-Diffusion">[code]</a>
                                    <p style="color:rgb(229, 118, 8);">
                                        Best Paper Award, ICRA 2023 Workshop on Effective Representations, Abstractions, and Priors for Robot Learning, <a href="https://sites.google.com/view/rap4robots">(RAP4Robots)</a>
                                    </p>
                                    <p></p>
                                    <p>
                                        We propose Tactile Diffusion to bridge the sim-to-real gap when using vision-based tactile sensors, like DIGIT. We demonstrate the utility of Tactile Diffusion on zero-shot clasification of Braille characters.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/ncf.gif' width="100%" />
                                    <!--<img src='images/mug_cut.gif' width="100%" />-->
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2210.09297">
                                        <papertitle>Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing</papertitle>
                                    </a>
                                    <br>
                                    <strong>Carolina Higuera*</strong>,
                                    <a href="https://sites.google.com/site/siyuandong000/">Siyuan Dong</a>,
                                    <a href="https://homes.cs.washington.edu/~bboots/">Byron Boots</a>,
                                    <a href="https://www.mustafamukadam.com/">Mustafa Mukadam</a>
                                    <br>
                                    <em>2023 International Conference on Robotics and Automation (ICRA)</em>
                                    <br>
                                    <a href="https://github.com/carolinahiguera/NCF">[code]</a>
                                    <br>
                                    <p></p>
                                    <p>
                                        Neural Contact Fields are an implicit representation for tracking extrinsic contact on an object surface (between object and environment) with vision-based tactile sensing (between robot hand and object).
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/microgrid.png' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://ieeexplore.ieee.org/abstract/document/9703072">
                                        <papertitle>Energy Management System for Microgrids based on Deep Reinforcement Learning </papertitle>
                                    </a>
                                    <br>
                                    <a href="">Cesar Garrido</a>,
                                    <a href="">Luis G. Marin</a>,
                                    <a href="">Guillermo Jiménez-Estévez</a>,
                                    <a href="https://academia.uniandes.edu.co/AcademyCv/flozano">Fernando Lozano</a>,
                                    <strong>Carolina Higuera</strong>,
                                    <br>
                                    <em>IEEE CHILECON</em>, 2021
                                    <br>
                                    <!-- <a href="https://anthonysimeonov.github.io/rpo-planning-framework/">project page</a> -->
                                    <br>
                                    <p></p>
                                    <!-- <p>We train conditional generative models to map object point clouds to a distribution over the contact and subgoal parameters of a set of manipulation skills, 
                                        and integrate these samplers into a framework for planning multistep manipulation of unknown rigid objects.
                                    </p> -->
                                    <p>
                                        Application of Deep RL for an Energy Management System (EMS) and its comparison with respect to classical techniques such as Rule-Based and Model Predictive Control.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/marl1.jpg' width="100%" />
                                    <!--<img src='images/mug_cut.gif' width="100%" />-->
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-24209-1_10">
                                        <papertitle>Multiagent Reinforcement Learning Applied to Traffic Light Signal Control</papertitle>
                                    </a>
                                    <br>
                                    <strong>Carolina Higuera</strong>,
                                    <a href="https://academia.uniandes.edu.co/AcademyCv/flozano">Fernando Lozano</a>,
                                    <a href="https://edgarcamilocamacho.github.io/">Edgar Camilo Camacho</a>,
                                    <a href="https://scholar.google.com/citations?user=lj5AMIsAAAAJ&hl=en">Carlos Higuera</a>
                                    <br>
                                    <em>PAAMS</em>, 2019 <!--(under review for <em>ICRA</em> 2022)-->
                                    <br>
                                    <a href="https://github.com/carolinahiguera/BogotaRL">Code</a> / <a href="https://youtu.be/A-Uv344e8Bc">Video</a> / <a href="">PDF</a>
                                    <br>
                                    <p></p>
                                    <p>
					Application of MARL to traffic light signal control to reduce travel time. We simulated a network with six signalized intersections in SUMO, using real data from the Transit Department of Bogota, Colombia. This project was my Master thesis!
                                    </p>
                                    <!-- <p>
					A representation that models objects as 3D neural fields of descriptors that encode category-level correspondence and are SE(3)-equivariant by construction. We apply this representation to enable pick-and-place on unseen objects in out-of-distribution poses from a small handful of demonstrations.
                                    </p> -->
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/traffic1.jpg' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-31321-0_34">
                                        <papertitle>An artificial Vision-Based Method for Vehicle Detection and Classification in Urban Traffic </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://edgarcamilocamacho.github.io/">Edgar Camilo Camacho</a>,
                                    <a href="">Cesar Pedraza</a>,
                                    <strong>Carolina Higuera</strong>,
                                    <br>
                                    <em>IbPRIA</em>, 2019
                                    <br>
                                    <!-- <a href="https://anthonysimeonov.github.io/rpo-planning-framework/">project page</a> -->
                                    <br>
                                    <p></p>
                                    <!-- <p>We train conditional generative models to map object point clouds to a distribution over the contact and subgoal parameters of a set of manipulation skills, 
                                        and integrate these samplers into a framework for planning multistep manipulation of unknown rigid objects.
                                    </p> -->
                                    <p>
                                        A system to analyze urban traffic using computer vision to get realiable information of traffic flow in Bogota, Colombia. 
                                    </p>
                                </td>
                            </tr>


                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Workshop Papers</heading>
                                    <p>
                                        <!-- I'm currently interested in machine learning for data-driven planning and control.
                                                                                Previously, I have studied the use of compliant materials as robot actuators.   -->
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                    
                            <tr>
                                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/ncf.gif' width="100%" />
                                </td> -->
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://openreview.net/forum?id=cqnuJ0rep_">
                                        <papertitle>Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing</papertitle>
                                    </a>
                                    <br>
                                    <strong>Carolina Higuera*</strong>,
                                    <a href="https://sites.google.com/site/siyuandong000/">Siyuan Dong</a>,
                                    <a href="https://homes.cs.washington.edu/~bboots/">Byron Boots</a>,
                                    <a href="https://www.mustafamukadam.com/">Mustafa Mukadam</a>
                                    <br>
                                    <em>CoRL 2022. Learning, Perception, and Abstraction for Long-Horizon Planning</em>
                                    <br>
                                    <a href="https://gjstein.github.io/corl2022wkshp-long-horizon-planning/">Workshop webpage</a>
                                    <br>
                                    <!-- <p></p> -->
                                    <!-- <p>
                                        Neural Contact Fields are an implicit representation for tracking extrinsic contact on an object surface (between object and environment) with vision-based tactile sensing (between robot hand and object).
                                    </p> -->
                                </td>
                            </tr>

                            <tr>
                                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/ncf.gif' width="100%" />
                                </td> -->
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="">
                                        <papertitle>Perceiving Extrinsic Contacts from Touch Improves Learning Insertion Policies</papertitle>
                                    </a>
                                    <br>
                                    <strong>Carolina Higuera*</strong>,
                                    <a>Joseph Ortiz</a>,
                                    <a>Haozhi Qi</a>
                                    <a>Luis Pineda</a>
                                    <a>Byron Boots</a>,
                                    <a>Mustafa Mukadam</a>
                                    <br>
                                    <em>CoRL 2023. 	Neural Representation Learning for Robot Manipulation</em>
                                    <br>
                                    <a href="https://neurl-rmw.github.io/">Workshop webpage</a>
                                    <br>
                                    <!-- <p></p> -->
                                    <!-- <p>
                                        Neural Contact Fields are an implicit representation for tracking extrinsic contact on an object surface (between object and environment) with vision-based tactile sensing (between robot hand and object).
                                    </p> -->
                                </td>
                            </tr>
                    
                    </table>
                    
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        <a href="https://github.com/jonbarron/jonbarron_website">template</a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                </td>
            </tr>
    </table>
</body>

</html>
